{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9_0JbD78SqP",
        "outputId": "e347fc21-9651-4f5d-d7e6-67d94134a61a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "100%|██████████| 14277/14277 [00:00<00:00, 166894.03it/s]\n",
            "100%|██████████| 3755/3755 [00:00<00:00, 156333.00it/s]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import os\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Parameters for BERT model and tokenization\n",
        "bert_path = \"bert-base-uncased\"  # Use the Hugging Face model\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
        "\n",
        "def get_df(file):\n",
        "    return pd.read_csv(file,sep = '\\t')\n",
        "\n",
        "train_df = get_df('train_posts.txt')\n",
        "test_df = get_df('test_posts.txt')\n",
        "\n",
        "def return_first_image(row):\n",
        "    return row['imageId(s)'].split(',')[0].strip()\n",
        "\n",
        "tqdm.pandas()\n",
        "train_df['first_image_id'] = train_df.progress_apply (lambda row: return_first_image(row),axis=1)\n",
        "test_df['first_image_id'] = test_df.progress_apply (lambda row: return_first_image(row),axis=1)\n",
        "\n",
        "from os import listdir\n",
        "\n",
        "images_train_dataset = [i for i in train_df['first_image_id'].tolist()]\n",
        "images_train_folder = [i.split('.')[0].strip() for i in listdir('images_train/')]\n",
        "images_train_not_available = set(images_train_dataset)-set(images_train_folder)\n",
        "images_train_not_available\n",
        "\n",
        "images_test_dataset = [i.split(',')[0].strip() for i in test_df['first_image_id'].tolist()]\n",
        "images_test_folder = [i.split('.')[0].strip() for i in listdir('images_test/')]\n",
        "images_test_not_available = set(images_test_dataset)-set(images_test_folder)\n",
        "images_test_not_available\n",
        "\n",
        "train_df = train_df[~train_df['first_image_id'].isin(images_train_not_available)]\n",
        "test_df = test_df[~test_df['first_image_id'].isin(images_test_not_available)]\n",
        "\n",
        "train_text = train_df['tweetText'].tolist()\n",
        "test_text = test_df['tweetText'].tolist()\n",
        "\n",
        "train_images = [i for i in train_df['first_image_id'].tolist()]\n",
        "test_images = [i.split(',')[0].strip() for i in test_df['imageId(s)'].tolist()]\n",
        "\n",
        "trainY = train_df['label'].tolist()\n",
        "trainY = [1 if i=='real' else 0 for i in trainY]\n",
        "\n",
        "testY = test_df['label'].tolist()\n",
        "testY = [1 if i=='real' else 0 for i in testY]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG7piJ_b9eac"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RNCGsmk7-4Sv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 14277/14277 [00:00<00:00, 181646.34it/s]\n",
            "100%|██████████| 3755/3755 [00:00<00:00, 177498.41it/s]\n"
          ]
        }
      ],
      "source": [
        "# def get_df(file):\n",
        "#     return pd.read_csv(file,sep = '\\t')\n",
        "\n",
        "# train_df = get_df('train_posts.txt')\n",
        "# test_df = get_df('test_posts.txt')\n",
        "\n",
        "# def return_first_image(row):\n",
        "#     return row['imageId(s)'].split(',')[0].strip()\n",
        "\n",
        "# tqdm.pandas()\n",
        "# train_df['first_image_id'] = train_df.progress_apply (lambda row: return_first_image(row),axis=1)\n",
        "# test_df['first_image_id'] = test_df.progress_apply (lambda row: return_first_image(row),axis=1)\n",
        "\n",
        "# from os import listdir\n",
        "\n",
        "# images_train_dataset = [i for i in train_df['first_image_id'].tolist()]\n",
        "# images_train_folder = [i.split('.')[0].strip() for i in listdir('images_train/')]\n",
        "# images_train_not_available = set(images_train_dataset)-set(images_train_folder)\n",
        "# images_train_not_available\n",
        "\n",
        "# images_test_dataset = [i.split(',')[0].strip() for i in test_df['first_image_id'].tolist()]\n",
        "# images_test_folder = [i.split('.')[0].strip() for i in listdir('images_test/')]\n",
        "# images_test_not_available = set(images_test_dataset)-set(images_test_folder)\n",
        "# images_test_not_available\n",
        "\n",
        "# train_df = train_df[~train_df['first_image_id'].isin(images_train_not_available)]\n",
        "# test_df = test_df[~test_df['first_image_id'].isin(images_test_not_available)]\n",
        "\n",
        "# train_text = train_df['tweetText'].tolist()\n",
        "# test_text = test_df['tweetText'].tolist()\n",
        "\n",
        "# train_images = [i for i in train_df['first_image_id'].tolist()]\n",
        "# test_images = [i.split(',')[0].strip() for i in test_df['imageId(s)'].tolist()]\n",
        "\n",
        "# trainY = train_df['label'].tolist()\n",
        "# trainY = [1 if i=='real' else 0 for i in trainY]\n",
        "\n",
        "# testY = test_df['label'].tolist()\n",
        "# testY = [1 if i=='real' else 0 for i in testY]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm5Nra9rCEBW",
        "outputId": "4e57f252-8772-4d7c-d2c5-aa2a24ebd96e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14258, 14258, 14258, 1923, 1923, 1923)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_text),len(train_images),len(trainY),len(test_text),len(test_images),len(testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fvk_XK49CG6L"
      },
      "outputs": [],
      "source": [
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "    return max([len(s.split()) for s in lines])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "IZqwtw80CIW7",
        "outputId": "e00f6d01-ef23-48d9-e6b9-04c2c72f1647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "maximum length: 901\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([   0.,    4.,  327.,  532.,  755.,  760.,  843.,  791.,  904.,\n",
              "        2534., 2379.,  775., 1462., 1113., 1047.,   17.]),\n",
              " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 12., 15., 16.,\n",
              "        18., 20., 25., 30.]),\n",
              " <BarContainer object of 16 artists>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"maximum length:\", max_length(train_text))\n",
        "plt.hist([len(s.split()) for s in train_text],bins=[0,1,2,3,4,5,6,7,8,9,12,15,16,18,20,25,30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSZA5BekCLfr",
        "outputId": "edf7a418-8820-4a30-9598-48380cd8cb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "128.0 14258\n",
            "0.008977416187403563\n"
          ]
        }
      ],
      "source": [
        "l=[len(s.split()) for s in train_text]\n",
        "count=0.0\n",
        "for i in l:\n",
        "    if i>22:\n",
        "        count+=1\n",
        "print(count,len(l))\n",
        "print(count/len(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tOgourlRCPAB"
      },
      "outputs": [],
      "source": [
        "max_seq_length=23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fQOmUJarCR14"
      },
      "outputs": [],
      "source": [
        "#trainY.count('real'),trainY.count('fake'),testY.count('real'),testY.count('fake')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuX6qzVlCaT_"
      },
      "source": [
        "# Text part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#ALBERT-base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting examples to features: 100%|██████████| 14258/14258 [00:05<00:00, 2743.01it/s]\n",
            "Converting examples to features: 100%|██████████| 1923/1923 [00:00<00:00, 2492.39it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from transformers import AlbertTokenizer\n",
        "from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "import torch\n",
        "\n",
        "class PaddingInputExample:\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\"\"\"\n",
        "    pass  # No need for methods or properties; just a marker class\n",
        "\n",
        "class InputExample:\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs an InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            label: (Optional) string. The label of the example.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer():\n",
        "    \"\"\"Instantiate the ALBERT tokenizer.\"\"\"\n",
        "    return AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label = 0\n",
        "        return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "\n",
        "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "\n",
        "    return (\n",
        "        torch.tensor(input_ids),          # Use torch tensors\n",
        "        torch.tensor(input_masks),        # Use torch tensors\n",
        "        torch.tensor(segment_ids),        # Use torch tensors\n",
        "        torch.tensor(labels).reshape(-1, 1)  # Use torch tensors\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    input_examples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        input_examples.append(\n",
        "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "        )\n",
        "    return input_examples\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = create_tokenizer()\n",
        "\n",
        "# Convert data to InputExample format\n",
        "# Assuming train_text and trainY, test_text and testY are already defined\n",
        "train_examples = convert_text_to_examples(train_text, trainY)\n",
        "test_examples = convert_text_to_examples(test_text, testY)\n",
        "\n",
        "# Convert to features\n",
        "train_input_ids, train_input_masks, train_segment_ids, trainY = convert_examples_to_features(tokenizer, train_examples, max_seq_length=256)\n",
        "test_input_ids, test_input_masks, test_segment_ids, testY = convert_examples_to_features(tokenizer, test_examples, max_seq_length=256)\n",
        "\n",
        "# Now you can proceed to use `train_input_ids`, `train_input_masks`, `train_segment_ids`, and `trainY`\n",
        "# for training your model in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsy9tbe1CaFr",
        "outputId": "570861b6-7474-44bb-b3bf-5e57ea8a75d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting examples to features: 100%|██████████| 14258/14258 [00:09<00:00, 1552.10it/s]\n",
            "Converting examples to features: 100%|██████████| 1923/1923 [00:01<00:00, 1287.85it/s]\n"
          ]
        }
      ],
      "source": [
        "# import numpy as np\n",
        "# from transformers import BertTokenizer\n",
        "# from tqdm import tqdm  # Import tqdm for progress tracking\n",
        "# import torch\n",
        "\n",
        "# class PaddingInputExample:\n",
        "#     \"\"\"Fake example so the num input examples is a multiple of the batch size.\"\"\"\n",
        "#     pass  # No need for methods or properties; just a marker class\n",
        "\n",
        "# class InputExample:\n",
        "#     \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "#     def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "#         \"\"\"Constructs an InputExample.\n",
        "\n",
        "#         Args:\n",
        "#             guid: Unique id for the example.\n",
        "#             text_a: string. The untokenized text of the first sequence.\n",
        "#             text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "#             label: (Optional) string. The label of the example.\n",
        "#         \"\"\"\n",
        "#         self.guid = guid\n",
        "#         self.text_a = text_a\n",
        "#         self.text_b = text_b\n",
        "#         self.label = label\n",
        "\n",
        "# def create_tokenizer():\n",
        "#     \"\"\"Instantiate the BERT tokenizer.\"\"\"\n",
        "#     return BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "#     \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "#     if isinstance(example, PaddingInputExample):\n",
        "#         input_ids = [0] * max_seq_length\n",
        "#         input_mask = [0] * max_seq_length\n",
        "#         segment_ids = [0] * max_seq_length\n",
        "#         label = 0\n",
        "#         return input_ids, input_mask, segment_ids, label\n",
        "\n",
        "#     tokens_a = tokenizer.tokenize(example.text_a)\n",
        "#     if len(tokens_a) > max_seq_length - 2:\n",
        "#         tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
        "\n",
        "#     tokens = []\n",
        "#     segment_ids = []\n",
        "#     tokens.append(\"[CLS]\")\n",
        "#     segment_ids.append(0)\n",
        "\n",
        "#     for token in tokens_a:\n",
        "#         tokens.append(token)\n",
        "#         segment_ids.append(0)\n",
        "\n",
        "#     tokens.append(\"[SEP]\")\n",
        "#     segment_ids.append(0)\n",
        "\n",
        "#     input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "#     # The mask has 1 for real tokens and 0 for padding tokens.\n",
        "#     input_mask = [1] * len(input_ids)\n",
        "\n",
        "#     # Zero-pad up to the sequence length.\n",
        "#     while len(input_ids) < max_seq_length:\n",
        "#         input_ids.append(0)\n",
        "#         input_mask.append(0)\n",
        "#         segment_ids.append(0)\n",
        "\n",
        "#     assert len(input_ids) == max_seq_length\n",
        "#     assert len(input_mask) == max_seq_length\n",
        "#     assert len(segment_ids) == max_seq_length\n",
        "\n",
        "#     return input_ids, input_mask, segment_ids, example.label\n",
        "\n",
        "# def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "#     \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "#     input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "\n",
        "#     for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
        "#         input_id, input_mask, segment_id, label = convert_single_example(\n",
        "#             tokenizer, example, max_seq_length\n",
        "#         )\n",
        "#         input_ids.append(input_id)\n",
        "#         input_masks.append(input_mask)\n",
        "#         segment_ids.append(segment_id)\n",
        "#         labels.append(label)\n",
        "\n",
        "#     return (\n",
        "#         torch.tensor(input_ids),          # Use torch tensors\n",
        "#         torch.tensor(input_masks),        # Use torch tensors\n",
        "#         torch.tensor(segment_ids),        # Use torch tensors\n",
        "#         torch.tensor(labels).reshape(-1, 1)  # Use torch tensors\n",
        "#     )\n",
        "\n",
        "# def convert_text_to_examples(texts, labels):\n",
        "#     \"\"\"Create InputExamples\"\"\"\n",
        "#     input_examples = []\n",
        "#     for text, label in zip(texts, labels):\n",
        "#         input_examples.append(\n",
        "#             InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
        "#         )\n",
        "#     return input_examples\n",
        "\n",
        "# # Instantiate the tokenizer\n",
        "# tokenizer = create_tokenizer()\n",
        "\n",
        "# # Convert data to InputExample format\n",
        "# # Assuming train_text and trainY, test_text and testY are already defined\n",
        "# train_examples = convert_text_to_examples(train_text, trainY)\n",
        "# test_examples = convert_text_to_examples(test_text, testY)\n",
        "\n",
        "# # Convert to features\n",
        "# train_input_ids, train_input_masks, train_segment_ids, trainY = convert_examples_to_features(tokenizer, train_examples, max_seq_length=256)\n",
        "# test_input_ids, test_input_masks, test_segment_ids, testY = convert_examples_to_features(tokenizer, test_examples, max_seq_length=256)\n",
        "\n",
        "# # Now you can proceed to use `train_input_ids`, `train_input_masks`, `train_segment_ids`, and `trainY`\n",
        "# # for training your model in PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB4AQ3AbEQ-u"
      },
      "source": [
        "# Image Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lPmDYmVREQwN"
      },
      "outputs": [],
      "source": [
        "length = 224\n",
        "width = 224\n",
        "channels = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4E878FuET2N"
      },
      "outputs": [],
      "source": [
        "# def read_and_process_image(list_of_images):\n",
        "#     X = []\n",
        "#     for image in tqdm(list_of_images):\n",
        "# #         print(image)\n",
        "#         if image is None:\n",
        "#             print(\"Error: Image not found or unable to load.\", image)\n",
        "\n",
        "#         else:\n",
        "#             print(\"Error: Image found.\", image)\n",
        "#             X.append(cv2.resize(cv2.imread(image, cv2.IMREAD_COLOR), (length,width), interpolation=cv2.INTER_CUBIC))\n",
        "\n",
        "#     return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVzBMfVCEV9a"
      },
      "outputs": [],
      "source": [
        "# images = listdir('images_train/')\n",
        "# images.extend(listdir('images_test/'))\n",
        "# jpg = []\n",
        "# png=[]\n",
        "# jpeg=[]\n",
        "# gif = []\n",
        "\n",
        "# for i in images:\n",
        "#     name,ext = i.split('.')[0],i.split('.')[-1]\n",
        "#     eval(ext).append(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuqVDoW8Emj3"
      },
      "outputs": [],
      "source": [
        "# def get_extension_of_file(file_name):\n",
        "#     if file_name in jpg:\n",
        "#         return '.jpg'\n",
        "#     elif file_name in png:\n",
        "#         return '.png'\n",
        "#     elif file_name in jpeg:\n",
        "#         return '.jpeg'\n",
        "#     else:\n",
        "#         return '.gif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lxOE9zbEnV5"
      },
      "outputs": [],
      "source": [
        "#train_images = [GOOGLE_DRIVE_PATH + '/images_train/'+i+get_extension_of_file(i) for i in train_images]\n",
        "#test_images = [GOOGLE_DRIVE_PATH + '/images_test/'+i+get_extension_of_file(i) for i in test_images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "002TGgAJMZjw"
      },
      "outputs": [],
      "source": [
        "#train_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaUULnm7EuPd"
      },
      "outputs": [],
      "source": [
        "#train_imagesX = read_and_process_image(train_images)\n",
        "#test_imagesX = read_and_process_image(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4jaFc7OIb4h"
      },
      "outputs": [],
      "source": [
        "# from google.colab.patches import cv2_imshow\n",
        "# image = cv2.imread('drive/My Drive/Deep Learning/Project/mediaeval2015/images_test/eclipse_video_01.gif')\n",
        "# cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiBOq5_sbeKD"
      },
      "outputs": [],
      "source": [
        "#np.save(GOOGLE_DRIVE_PATH+'train_imagesX.npy', train_imagesX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy--PkuzhXt4"
      },
      "outputs": [],
      "source": [
        "#np.save(GOOGLE_DRIVE_PATH+'test_imagesX.npy', test_imagesX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pd9368hQbd-H"
      },
      "outputs": [],
      "source": [
        "train_imagesX = np.load('train_imagesX.npy')\n",
        "test_imagesX = np.load('test_imagesX.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PbxOQgMCiZsb"
      },
      "outputs": [],
      "source": [
        "train_imagesX = np.rollaxis(train_imagesX, 3, 1)\n",
        "test_imagesX = np.rollaxis(test_imagesX,3,1)\n",
        "\n",
        "# Alternatively, using from_numpy which preserves the original data type\n",
        "train_imagesX = torch.from_numpy(train_imagesX).float()\n",
        "test_imagesX  = torch.from_numpy(test_imagesX).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qlHSdxhicoD"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AlbertModel\n",
        "\n",
        "class AlbertLayer(nn.Module):\n",
        "    def __init__(self, albert_path, n_fine_tune_layers=1):\n",
        "        super(AlbertLayer, self).__init__()\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.output_size = 256  # Target output size after dimensionality reduction\n",
        "        \n",
        "        # Load pre-trained ALBERT model\n",
        "        self.albert = AlbertModel.from_pretrained(albert_path)\n",
        "        \n",
        "        # Set all layers to non-trainable initially\n",
        "        for param in self.albert.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        # Unfreeze the last `n_fine_tune_layers` layers\n",
        "        if n_fine_tune_layers > 0:\n",
        "            for layer in self.albert.encoder.albert_layer_groups[-n_fine_tune_layers:]:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "    \n",
        "        # Linear layer to reduce output size from 768 to 256\n",
        "        self.fc = nn.Linear(768, self.output_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        # Get the pooled output from ALBERT\n",
        "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        \n",
        "        # Extract pooled output (representation of [CLS] token)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        # Reduce the dimensionality\n",
        "        reduced_output = self.fc(pooled_output)\n",
        "        \n",
        "        return reduced_output\n",
        "\n",
        "# Example usage:\n",
        "albert_layer = AlbertLayer(albert_path=\"albert-base-v2\", n_fine_tune_layers=10)\n",
        "input_ids = torch.tensor([[101, 2009, 2003, 1037, 3071, 102]])  # Example input\n",
        "attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1]])\n",
        "output = albert_layer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "print(output.shape)  # Should print torch.Size([1, 256])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from transformers import BertModel\n",
        "\n",
        "# class BertLayer(nn.Module):\n",
        "#     def __init__(self, bert_path, n_fine_tune_layers=10):\n",
        "#         super(BertLayer, self).__init__()\n",
        "#         self.n_fine_tune_layers = n_fine_tune_layers\n",
        "#         self.output_size = 256  # BERT-Base output size=768\n",
        "        \n",
        "#         # Load pre-trained BERT model\n",
        "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "#         # Set all layers to non-trainable initially\n",
        "#         for param in self.bert.parameters():\n",
        "#             param.requires_grad = False\n",
        "            \n",
        "#         # Unfreeze the last `n_fine_tune_layers` layers\n",
        "#         if n_fine_tune_layers > 0:\n",
        "#             for layer in self.bert.encoder.layer[-n_fine_tune_layers:]:\n",
        "#                 for param in layer.parameters():\n",
        "#                     param.requires_grad = True\n",
        "    \n",
        "#         # Linear layer to reduce output size from 768 to 256\n",
        "#         self.fc = nn.Linear(768, self.output_size)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "#         # Get the pooled output from BERT\n",
        "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        \n",
        "#         # Extract pooled output (representation of [CLS] token)\n",
        "#         pooled_output = outputs.pooler_output\n",
        "#         # Reduce the dimensionality\n",
        "#         reduced_output = self.fc(pooled_output)\n",
        "        \n",
        "#         return reduced_output\n",
        "\n",
        "# #Example usage:\n",
        "# bert_layer = BertLayer(bert_path=\"bert-base-uncased\", n_fine_tune_layers=10)\n",
        "# input_ids = torch.tensor([[101, 2009, 2003, 1037, 3071, 102]])  # Example input\n",
        "# attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1]])\n",
        "# output = bert_layer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "# print(output.shape)  # Should print torch.Size([1, 768])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from transformers import MobileBertModel\n",
        "\n",
        "# class BertLayer(nn.Module):\n",
        "#     def __init__(self, bert_path=\"google/mobilebert-uncased\", n_fine_tune_layers=2):  #n_fine_tune_layers=10\n",
        "#         super(BertLayer, self).__init__()\n",
        "#         self.n_fine_tune_layers = n_fine_tune_layers\n",
        "#         self.output_size = 768  # MobileBERT output size is 768\n",
        "        \n",
        "#         # Load pre-trained MobileBERT model\n",
        "#         self.bert = MobileBertModel.from_pretrained(bert_path)\n",
        "        \n",
        "#         # Set all layers to non-trainable initially\n",
        "#         for param in self.bert.parameters():\n",
        "#             param.requires_grad = False\n",
        "            \n",
        "#         # Unfreeze the last `n_fine_tune_layers` layers\n",
        "#         if n_fine_tune_layers > 0:\n",
        "#             for layer in self.bert.encoder.layer[-n_fine_tune_layers:]:\n",
        "#                 for param in layer.parameters():\n",
        "#                     param.requires_grad = True\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "#         # Get the pooled output from MobileBERT\n",
        "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        \n",
        "#         # Extract pooled output (representation of [CLS] token)\n",
        "#         pooled_output = outputs.pooler_output\n",
        "        \n",
        "#         return pooled_output\n",
        "\n",
        "# # Example usage:\n",
        "# if __name__ == \"__main__\":\n",
        "#     bert_layer = BertLayer(n_fine_tune_layers=10)\n",
        "#     input_ids = torch.tensor([[101, 2009, 2003, 1037, 3071, 102]])  # Example input\n",
        "#     attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1]])\n",
        "    \n",
        "#     output = bert_layer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#     print(output.shape)  # Should print torch.Size([1, 768])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUdlZvRi8oqe"
      },
      "source": [
        "Only to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torchvision import models\n",
        "# # Define the main NewsModel\n",
        "# class NewsModel(nn.Module):\n",
        "#     def __init__(self, params):\n",
        "#         super(NewsModel, self).__init__()\n",
        "        \n",
        "#         # Initialize the BertLayer\n",
        "#         self.bert_layer = BertLayer(bert_path=params[\"bert_path\"], \n",
        "#                                     n_fine_tune_layers=params['n_fine_tune_layers'])\n",
        "        \n",
        "#         # Text model layers (if additional hidden layers are specified)\n",
        "#         text_hidden_layers = []\n",
        "#         input_size = self.bert_layer.output_size\n",
        "#         for _ in range(params['text_no_hidden_layer']):\n",
        "#             text_hidden_layers.append(nn.Linear(input_size, params['text_hidden_neurons']))\n",
        "#             text_hidden_layers.append(nn.ReLU())\n",
        "#             text_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "#             input_size = params['text_hidden_neurons']\n",
        "#         self.text_hidden_layers = nn.Sequential(*text_hidden_layers)\n",
        "        \n",
        "#         self.text_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "#         # Image model with VGG-19 feature extractor\n",
        "#         self.vgg_base = models.vgg19(pretrained=True)\n",
        "#         self.vgg_base.classifier = nn.Identity()  # Remove classifier for feature extraction\n",
        "#         for param in self.vgg_base.parameters():\n",
        "#             param.requires_grad = False  # Freeze VGG base layers\n",
        "        \n",
        "#         # Additional hidden layers for image model (if specified)\n",
        "#         vis_hidden_layers = []\n",
        "#         input_size = 512 * 7 * 7  # Flattened output from VGG19's feature extractor\n",
        "#         for _ in range(params['vis_no_hidden_layer']):\n",
        "#             vis_hidden_layers.append(nn.Linear(input_size, params['vis_hidden_neurons']))\n",
        "#             vis_hidden_layers.append(nn.ReLU())\n",
        "#             vis_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "#             input_size = params['vis_hidden_neurons']\n",
        "#         self.vis_hidden_layers = nn.Sequential(*vis_hidden_layers)\n",
        "\n",
        "#         self.visual_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "#         # Final combined classifier\n",
        "#         combined_size = 2 * params['repr_size']\n",
        "#         final_hidden_layers = []\n",
        "#         for _ in range(params['final_no_hidden_layer']):\n",
        "#             final_hidden_layers.append(nn.Linear(combined_size, params['final_hidden_neurons']))\n",
        "#             final_hidden_layers.append(nn.ReLU())\n",
        "#             final_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "#             combined_size = params['final_hidden_neurons']\n",
        "#         self.final_hidden_layers = nn.Sequential(*final_hidden_layers)\n",
        "\n",
        "#         self.classifier = nn.Linear(combined_size, 1)  # Binary classification\n",
        "    \n",
        "#     def forward(self, input_ids, attention_mask, token_type_ids, image):\n",
        "#         # BERT forward pass\n",
        "#         text_output = self.bert_layer(input_ids, attention_mask, token_type_ids)\n",
        "#         text_output = self.text_hidden_layers(text_output)\n",
        "#         text_repr = self.text_repr(text_output)\n",
        "        \n",
        "#         # VGG-19 forward pass\n",
        "#         image_features = self.vgg_base(image)\n",
        "#         image_flatten = image_features.view(image_features.size(0), -1)  # Flatten for fully connected layers\n",
        "#         image_output = self.vis_hidden_layers(image_flatten)\n",
        "#         visual_repr = self.visual_repr(image_output)\n",
        "        \n",
        "#         # Concatenate text and visual representations\n",
        "#         combined_repr = torch.cat((text_repr, visual_repr), dim=1)\n",
        "#         combined_output = self.final_hidden_layers(combined_repr)\n",
        "        \n",
        "#         # Final classifier layer\n",
        "#         prediction = torch.sigmoid(self.classifier(combined_output))\n",
        "#         print(f\"Prediction shape: {prediction.shape}\")  # Debugging line\n",
        "#         return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "# Define the main NewsModel\n",
        "class NewsModel(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(NewsModel, self).__init__()\n",
        "        \n",
        "        # Initialize the BertLayer\n",
        "        self.bert_layer = AlbertLayer(albert_path=\"albert-base-v2\", \n",
        "                                    n_fine_tune_layers=params['n_fine_tune_layers'])\n",
        "        \n",
        "        # Text model layers (if additional hidden layers are specified)\n",
        "        text_hidden_layers = []\n",
        "        input_size = self.bert_layer.output_size\n",
        "        for _ in range(params['text_no_hidden_layer']):\n",
        "            text_hidden_layers.append(nn.Linear(input_size, params['text_hidden_neurons']))\n",
        "            text_hidden_layers.append(nn.ReLU())\n",
        "            text_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['text_hidden_neurons']\n",
        "        self.text_hidden_layers = nn.Sequential(*text_hidden_layers)\n",
        "        \n",
        "        self.text_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # Image model with MobileNet feature extractor\n",
        "        self.mobilenet_base = models.mobilenet_v2(pretrained=True)  # Use MobileNetV2\n",
        "        self.mobilenet_base.classifier = nn.Identity()  # Remove classifier for feature extraction\n",
        "        for param in self.mobilenet_base.parameters():\n",
        "            param.requires_grad = False  # Freeze MobileNet base layers\n",
        "        \n",
        "        # Additional hidden layers for image model (if specified)\n",
        "        vis_hidden_layers = []\n",
        "        input_size = 1280  # Flattened output from MobileNetV2\n",
        "        for _ in range(params['vis_no_hidden_layer']):\n",
        "            vis_hidden_layers.append(nn.Linear(input_size, params['vis_hidden_neurons']))\n",
        "            vis_hidden_layers.append(nn.ReLU())\n",
        "            vis_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['vis_hidden_neurons']\n",
        "        self.vis_hidden_layers = nn.Sequential(*vis_hidden_layers)\n",
        "\n",
        "        self.visual_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # Final combined classifier\n",
        "        combined_size = 2 * params['repr_size']\n",
        "        final_hidden_layers = []\n",
        "        for _ in range(params['final_no_hidden_layer']):\n",
        "            final_hidden_layers.append(nn.Linear(combined_size, params['final_hidden_neurons']))\n",
        "            final_hidden_layers.append(nn.ReLU())\n",
        "            final_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            combined_size = params['final_hidden_neurons']\n",
        "        self.final_hidden_layers = nn.Sequential(*final_hidden_layers)\n",
        "\n",
        "        self.classifier = nn.Linear(combined_size, 1)  # Binary classification\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, image):\n",
        "        # BERT forward pass\n",
        "        text_output = self.bert_layer(input_ids, attention_mask, token_type_ids)\n",
        "        text_output = self.text_hidden_layers(text_output)\n",
        "        text_repr = self.text_repr(text_output)\n",
        "        \n",
        "        # VGG-19 forward pass\n",
        "        image_features = self.mobilenet_base(image)\n",
        "        #image_flatten = image_features.view(image_features.size(0), -1)  # Flatten for fully connected layers\n",
        "        #image_output = self.vis_hidden_layers(image_flatten)\n",
        "        image_output = self.vis_hidden_layers(image_features )\n",
        "        visual_repr = self.visual_repr(image_output)\n",
        "        \n",
        "        # Concatenate text and visual representations\n",
        "        combined_repr = torch.cat((text_repr, visual_repr), dim=1)\n",
        "        combined_output = self.final_hidden_layers(combined_repr)\n",
        "        \n",
        "        # Final classifier layer\n",
        "        prediction = torch.sigmoid(self.classifier(combined_output))\n",
        "        \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    'albert_path':\"albert-base-v2\",\n",
        "    'n_fine_tune_layers':1,\n",
        "    'bert_trainable' :False,\n",
        "    'text_no_hidden_layer':1,\n",
        "    'text_hidden_neurons':64,   #'text_hidden_neurons':768,\n",
        "    'dropout':0.4,\n",
        "    'repr_size':32,\n",
        "    'vis_no_hidden_layer':1,\n",
        "    'vis_hidden_neurons':64,   #'vis_hidden_neurons':2742,\n",
        "    'final_no_hidden_layer':1,\n",
        "    'final_hidden_neurons':35,\n",
        "    'optimizer':['adam'],\n",
        "    'batch_size':[1],\n",
        "    'epochs':[10]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is CUDA available? True\n",
            "Current GPU device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "is_cuda_available = torch.cuda.is_available()\n",
        "print(f\"Is CUDA available? {is_cuda_available}\")\n",
        "\n",
        "# If CUDA is available, get the current device\n",
        "if is_cuda_available:\n",
        "    current_device = torch.cuda.current_device()\n",
        "    device_name = torch.cuda.get_device_name(current_device)\n",
        "    print(f\"Current GPU device: {device_name}\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.48 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.72 GiB is allocated by PyTorch, and 107.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Run the model with a fixed learning rate (you can set the desired learning rate here)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m fixed_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Set the learning rate you want to test\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mlr_finder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_segment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_imagesX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# After running the model, you can visualize the results if needed\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[16], line 35\u001b[0m, in \u001b[0;36mLRFinder.run\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, images, labels, lr)\u001b[0m\n\u001b[0;32m     26\u001b[0m input_ids, attention_mask, token_type_ids, images, labels \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     27\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     28\u001b[0m     attention_mask\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39msqueeze())\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[14], line 53\u001b[0m, in \u001b[0;36mNewsModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, image)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, token_type_ids, image):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# BERT forward pass\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_hidden_layers(text_output)\n\u001b[0;32m     55\u001b[0m     text_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_repr(text_output)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mAlbertLayer.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Get the pooled output from ALBERT\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Extract pooled output (representation of [CLS] token)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:794\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    786\u001b[0m use_sdpa_attention_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions\n\u001b[0;32m    791\u001b[0m )\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_sdpa_attention_mask:\n\u001b[1;32m--> 794\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:447\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:188\u001b[0m, in \u001b[0;36mAttentionMaskConverter._expand_mask\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    184\u001b[0m expanded_mask \u001b[38;5;241m=\u001b[39m mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mexpand(bsz, \u001b[38;5;241m1\u001b[39m, tgt_len, src_len)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[0;32m    186\u001b[0m inverted_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m expanded_mask\n\u001b[1;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minverted_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverted_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.48 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.72 GiB is allocated by PyTorch, and 107.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "class LRFinder:\n",
        "    def __init__(self, model, criterion=nn.BCELoss(), optimizer=None):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.history = {\n",
        "            \"lr\": [],\n",
        "            \"loss\": []\n",
        "        }\n",
        "\n",
        "    def run(self, input_ids, attention_mask, token_type_ids, images, labels, \n",
        "            lr):\n",
        "        # Put model in training mode\n",
        "        self.model.train()\n",
        "\n",
        "        # Setup optimizer if not already provided\n",
        "        if self.optimizer is None:\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # Move inputs to device\n",
        "        device = next(self.model.parameters()).device\n",
        "        input_ids, attention_mask, token_type_ids, images, labels = (\n",
        "            input_ids.to(device),\n",
        "            attention_mask.to(device),\n",
        "            token_type_ids.to(device),\n",
        "            images.to(device),\n",
        "            labels.float().to(device)\n",
        "        )\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(input_ids, attention_mask, token_type_ids, images)\n",
        "        outputs = outputs.unsqueeze(1)\n",
        "        loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
        "\n",
        "        # Record the loss and learning rate\n",
        "        self.history['lr'].append(lr)\n",
        "        self.history['loss'].append(loss.item())\n",
        "\n",
        "        # Perform backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Learning Rate: {lr:.2e}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have prepared your training data: train_input_ids, train_input_masks, train_segment_ids, train_imagesX, trainY\n",
        "\n",
        "# Create the model\n",
        "model_adam = NewsModel(params).to(current_device)\n",
        "\n",
        "# Initialize the learning rate finder\n",
        "lr_finder = LRFinder(model_adam)\n",
        "\n",
        "# Run the model with a fixed learning rate (you can set the desired learning rate here)\n",
        "fixed_lr = 0.001  # Set the learning rate you want to test\n",
        "lr_finder.run(train_input_ids, train_input_masks, train_segment_ids, train_imagesX, trainY, \n",
        "               lr=fixed_lr)\n",
        "\n",
        "# After running the model, you can visualize the results if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import torch.optim as optim\n",
        "\n",
        "# class LRFinder:\n",
        "#     def __init__(self, model, criterion=nn.BCELoss(), optimizer=None):\n",
        "#         self.model = model\n",
        "#         self.criterion = criterion\n",
        "#         self.optimizer = optimizer\n",
        "#         self.history = {\n",
        "#             \"lr\": [],\n",
        "#             \"loss\": []\n",
        "#         }\n",
        "\n",
        "#     def find(self, input_ids, attention_mask, token_type_ids, images, labels, \n",
        "#              start_lr, end_lr, num_iter, beta=0.98):\n",
        "#         # Put model in training mode\n",
        "#         self.model.train()\n",
        "\n",
        "#         # Setup optimizer if not already provided\n",
        "#         if self.optimizer is None:\n",
        "#             self.optimizer = optim.Adam(self.model.parameters(), lr=start_lr)\n",
        "\n",
        "#         # Clear the gradients\n",
        "#         self.optimizer.zero_grad()\n",
        "\n",
        "#         # Calculate the total number of iterations\n",
        "#         num_samples = len(labels)\n",
        "#         iteration = 0\n",
        "        \n",
        "#         # Move inputs to device\n",
        "#         device = next(self.model.parameters()).device\n",
        "#         input_ids, attention_mask, token_type_ids, images, labels = (\n",
        "#             input_ids.to(device),\n",
        "#             attention_mask.to(device),\n",
        "#             token_type_ids.to(device),\n",
        "#             images.to(device),\n",
        "#             labels.float().to(device)\n",
        "#         )\n",
        "\n",
        "#         # Learning rate schedule\n",
        "#         lr_schedule = np.logspace(np.log10(start_lr), np.log10(end_lr), num_iter)\n",
        "\n",
        "#         for lr in lr_schedule:\n",
        "#             self.optimizer.param_groups[0]['lr'] = lr\n",
        "#             outputs = self.model(input_ids, attention_mask, token_type_ids, images)\n",
        "#             loss = self.criterion(outputs.squeeze(), labels)\n",
        "\n",
        "#             # Record the loss and learning rate\n",
        "#             self.history['lr'].append(lr)\n",
        "#             self.history['loss'].append(loss.item())\n",
        "\n",
        "#             # Perform backpropagation\n",
        "#             self.optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             self.optimizer.step()\n",
        "\n",
        "#             # Optionally print progress\n",
        "#             if iteration % (num_iter // 10) == 0:\n",
        "#                 print(f\"Iteration {iteration}/{num_iter}: lr={lr:.2e}, loss={loss.item():.4f}\")\n",
        "\n",
        "#             iteration += 1\n",
        "        \n",
        "#         print(\"Finished LR Finder.\")\n",
        "\n",
        "# # Example usage\n",
        "# # Assuming you have prepared your training data: train_input_ids, train_input_masks, train_segment_ids, train_imagesX, trainY\n",
        "\n",
        "# # Create the model\n",
        "# model_adam = NewsModel(params).to(current_device)\n",
        "\n",
        "# # Initialize the learning rate finder\n",
        "# lr_finder = LRFinder(model_adam)\n",
        "\n",
        "# # Find the optimal learning rate\n",
        "# lr_finder.find(train_input_ids, train_input_masks, train_segment_ids, train_imagesX, trainY,\n",
        "#                 start_lr=0.000001, end_lr=0.01, num_iter=128)\n",
        "\n",
        "# # After running the LR Finder, you can plot the results to visualize the optimal learning rate\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(lr_finder.history['lr'], lr_finder.history['loss'])\n",
        "# plt.xscale('log')\n",
        "# plt.xlabel('Learning Rate')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.title('Learning Rate Finder')\n",
        "# plt.grid()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:404: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attention_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "labels shape: torch.Size([1, 1])\n",
            "Learning Rate: 1.00e-03, Loss: 0.6586\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "class LRFinder:\n",
        "    def __init__(self, model, criterion=nn.BCELoss(), optimizer=None):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.history = {\n",
        "            \"lr\": [],\n",
        "            \"loss\": []\n",
        "        }\n",
        "\n",
        "    def run(self, input_ids, attention_mask, token_type_ids, images, labels, lr):\n",
        "        # Put model in training mode\n",
        "        self.model.train()\n",
        "\n",
        "        # Setup optimizer if not already provided\n",
        "        if self.optimizer is None:\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # Move inputs to device\n",
        "        device = next(self.model.parameters()).device\n",
        "        input_ids, attention_mask, token_type_ids, images, labels = (\n",
        "            input_ids.to(device),\n",
        "            attention_mask.to(device),\n",
        "            token_type_ids.to(device),\n",
        "            images.to(device),\n",
        "            labels.float().to(device)  # Ensure labels are float for BCELoss\n",
        "        )\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(input_ids, attention_mask, token_type_ids, images)\n",
        "        outputs = outputs.unsqueeze(1)\n",
        "        print(f\"labels shape: {labels.shape}\")  # Debugging line\n",
        "        loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
        "\n",
        "        # Record the loss and learning rate\n",
        "        self.history['lr'].append(lr)\n",
        "        self.history['loss'].append(loss.item())\n",
        "\n",
        "        # Perform backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Learning Rate: {lr:.2e}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Assuming you have prepared your training data: train_input_ids, train_input_masks, train_segment_ids, train_imagesX, trainY\n",
        "\n",
        "# Create the model\n",
        "model_adam = NewsModel(params).to(current_device)\n",
        "\n",
        "# Initialize the learning rate finder\n",
        "lr_finder = LRFinder(model_adam)\n",
        "\n",
        "# Run the model with a fixed learning rate (you can set the desired learning rate here)\n",
        "fixed_lr = 0.001  # Set the learning rate you want to test\n",
        "try:\n",
        "    lr_finder.run(train_input_ids[0:1], train_input_masks[0:1], train_segment_ids[0:1], train_imagesX[0:1], trainY[0:1], \n",
        "                   lr=fixed_lr)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters in the model: 132958343\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class NewsModel(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(NewsModel, self).__init__()\n",
        "        \n",
        "        # Initialize the BertLayer (assuming it's defined)\n",
        "        self.bert_layer = BertLayer(bert_path=params[\"bert_path\"], \n",
        "                                    n_fine_tune_layers=params['n_fine_tune_layers'])\n",
        "        \n",
        "        # Text model layers\n",
        "        text_hidden_layers = []\n",
        "        input_size = self.bert_layer.output_size\n",
        "        for _ in range(params['text_no_hidden_layer']):\n",
        "            text_hidden_layers.append(nn.Linear(input_size, params['text_hidden_neurons']))\n",
        "            text_hidden_layers.append(nn.ReLU())\n",
        "            text_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['text_hidden_neurons']\n",
        "        self.text_hidden_layers = nn.Sequential(*text_hidden_layers)\n",
        "        \n",
        "        self.text_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # VGG-19 feature extractor\n",
        "        self.vgg_base = models.vgg19(pretrained=True)\n",
        "        self.vgg_base.classifier = nn.Identity()  # Remove classifier for feature extraction\n",
        "        for param in self.vgg_base.parameters():\n",
        "            param.requires_grad = False  # Freeze VGG base layers\n",
        "        \n",
        "        # Image model layers\n",
        "        vis_hidden_layers = []\n",
        "        input_size = 512 * 7 * 7  # Flattened output from VGG19's feature extractor\n",
        "        for _ in range(params['vis_no_hidden_layer']):\n",
        "            vis_hidden_layers.append(nn.Linear(input_size, params['vis_hidden_neurons']))\n",
        "            vis_hidden_layers.append(nn.ReLU())\n",
        "            vis_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['vis_hidden_neurons']\n",
        "        self.vis_hidden_layers = nn.Sequential(*vis_hidden_layers)\n",
        "\n",
        "        self.visual_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # Final classifier\n",
        "        combined_size = 2 * params['repr_size']\n",
        "        final_hidden_layers = []\n",
        "        for _ in range(params['final_no_hidden_layer']):\n",
        "            final_hidden_layers.append(nn.Linear(combined_size, params['final_hidden_neurons']))\n",
        "            final_hidden_layers.append(nn.ReLU())\n",
        "            final_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            combined_size = params['final_hidden_neurons']\n",
        "        self.final_hidden_layers = nn.Sequential(*final_hidden_layers)\n",
        "\n",
        "        self.classifier = nn.Linear(combined_size, 1)  # Binary classification\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, image):\n",
        "        # BERT forward pass\n",
        "        text_output = self.bert_layer(input_ids, attention_mask, token_type_ids)\n",
        "        text_output = self.text_hidden_layers(text_output)\n",
        "        text_repr = self.text_repr(text_output)\n",
        "        \n",
        "        # VGG-19 forward pass\n",
        "        image_features = self.vgg_base(image)\n",
        "        image_flatten = image_features.view(image_features.size(0), -1)  # Flatten for fully connected layers\n",
        "        image_output = self.vis_hidden_layers(image_flatten)\n",
        "        visual_repr = self.visual_repr(image_output)\n",
        "        \n",
        "        # Concatenate text and visual representations\n",
        "        combined_repr = torch.cat((text_repr, visual_repr), dim=1)\n",
        "        combined_output = self.final_hidden_layers(combined_repr)\n",
        "        \n",
        "        # Final classifier layer\n",
        "        prediction = torch.sigmoid(self.classifier(combined_output))\n",
        "        return prediction\n",
        "\n",
        "    def total_params(self):\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "# Example parameters (modify as needed)\n",
        "params = {\n",
        "    'bert_path':'bert-base-uncased',\n",
        "    'n_fine_tune_layers':2,\n",
        "    'bert_trainable' :False,\n",
        "    'text_no_hidden_layer':1,\n",
        "    'text_hidden_neurons':128,   #'text_hidden_neurons':768,\n",
        "    'dropout':0.4,\n",
        "    'repr_size':32,\n",
        "    'vis_no_hidden_layer':1,\n",
        "    'vis_hidden_neurons':128,   #'vis_hidden_neurons':2742,\n",
        "    'final_no_hidden_layer':1,\n",
        "    'final_hidden_neurons':35,\n",
        "    'optimizer':['adam'],\n",
        "    'batch_size':[8],\n",
        "    'epochs':[10]\n",
        "}\n",
        "model = NewsModel(params)\n",
        "print(f'Total number of parameters in the model: {model.total_params()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters in the model: 14311751\n"
          ]
        }
      ],
      "source": [
        "class NewsModel(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(NewsModel, self).__init__()\n",
        "        \n",
        "        # Initialize the BertLayer\n",
        "        self.bert_layer = AlbertLayer(albert_path=\"albert-base-v2\", \n",
        "                                    n_fine_tune_layers=params['n_fine_tune_layers'])\n",
        "        \n",
        "        # Text model layers (if additional hidden layers are specified)\n",
        "        text_hidden_layers = []\n",
        "        input_size = self.bert_layer.output_size\n",
        "        for _ in range(params['text_no_hidden_layer']):\n",
        "            text_hidden_layers.append(nn.Linear(input_size, params['text_hidden_neurons']))\n",
        "            text_hidden_layers.append(nn.ReLU())\n",
        "            text_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['text_hidden_neurons']\n",
        "        self.text_hidden_layers = nn.Sequential(*text_hidden_layers)\n",
        "        \n",
        "        self.text_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # Image model with MobileNet feature extractor\n",
        "        self.mobilenet_base = models.mobilenet_v2(pretrained=True)  # Use MobileNetV2\n",
        "        self.mobilenet_base.classifier = nn.Identity()  # Remove classifier for feature extraction\n",
        "        for param in self.mobilenet_base.parameters():\n",
        "            param.requires_grad = False  # Freeze MobileNet base layers\n",
        "        \n",
        "        # Additional hidden layers for image model (if specified)\n",
        "        vis_hidden_layers = []\n",
        "        input_size = 1280  # Flattened output from MobileNetV2\n",
        "        for _ in range(params['vis_no_hidden_layer']):\n",
        "            vis_hidden_layers.append(nn.Linear(input_size, params['vis_hidden_neurons']))\n",
        "            vis_hidden_layers.append(nn.ReLU())\n",
        "            vis_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['vis_hidden_neurons']\n",
        "        self.vis_hidden_layers = nn.Sequential(*vis_hidden_layers)\n",
        "\n",
        "        self.visual_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # Final combined classifier\n",
        "        combined_size = 2 * params['repr_size']\n",
        "        final_hidden_layers = []\n",
        "        for _ in range(params['final_no_hidden_layer']):\n",
        "            final_hidden_layers.append(nn.Linear(combined_size, params['final_hidden_neurons']))\n",
        "            final_hidden_layers.append(nn.ReLU())\n",
        "            final_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            combined_size = params['final_hidden_neurons']\n",
        "        self.final_hidden_layers = nn.Sequential(*final_hidden_layers)\n",
        "\n",
        "        self.classifier = nn.Linear(combined_size, 1)  # Binary classification\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, image):\n",
        "        # BERT forward pass\n",
        "        text_output = self.bert_layer(input_ids, attention_mask, token_type_ids)\n",
        "        text_output = self.text_hidden_layers(text_output)\n",
        "        text_repr = self.text_repr(text_output)\n",
        "        \n",
        "        # VGG-19 forward pass\n",
        "        image_features = self.mobilenet_base(image)\n",
        "        #image_flatten = image_features.view(image_features.size(0), -1)  # Flatten for fully connected layers\n",
        "        #image_output = self.vis_hidden_layers(image_flatten)\n",
        "        image_output = self.vis_hidden_layers(image_features )\n",
        "        visual_repr = self.visual_repr(image_output)\n",
        "        \n",
        "        # Concatenate text and visual representations\n",
        "        combined_repr = torch.cat((text_repr, visual_repr), dim=1)\n",
        "        combined_output = self.final_hidden_layers(combined_repr)\n",
        "        \n",
        "        # Final classifier layer\n",
        "        prediction = torch.sigmoid(self.classifier(combined_output))\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    def total_params(self):\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "# Example parameters (modify as needed)\n",
        "params = {\n",
        "    'bert_path':'bert-base-uncased',\n",
        "    'n_fine_tune_layers':2,\n",
        "    'bert_trainable' :False,\n",
        "    'text_no_hidden_layer':1,\n",
        "    'text_hidden_neurons':128,   #'text_hidden_neurons':768,\n",
        "    'dropout':0.4,\n",
        "    'repr_size':32,\n",
        "    'vis_no_hidden_layer':1,\n",
        "    'vis_hidden_neurons':128,   #'vis_hidden_neurons':2742,\n",
        "    'final_no_hidden_layer':1,\n",
        "    'final_hidden_neurons':35,\n",
        "    'optimizer':['adam'],\n",
        "    'batch_size':[8],\n",
        "    'epochs':[10]\n",
        "}\n",
        "model = NewsModel(params)\n",
        "print(f'Total number of parameters in the model: {model.total_params()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters in the model: 14209223\n"
          ]
        }
      ],
      "source": [
        "class NewsModel(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(NewsModel, self).__init__()\n",
        "        \n",
        "        # Initialize the BertLayer\n",
        "        self.bert_layer = AlbertLayer(albert_path=\"albert-base-v2\", \n",
        "                                    n_fine_tune_layers=params['n_fine_tune_layers'])\n",
        "        \n",
        "        # Text model layers (if additional hidden layers are specified)\n",
        "        text_hidden_layers = []\n",
        "        input_size = self.bert_layer.output_size\n",
        "        for _ in range(params['text_no_hidden_layer']):\n",
        "            text_hidden_layers.append(nn.Linear(input_size, params['text_hidden_neurons']))\n",
        "            text_hidden_layers.append(nn.ReLU())\n",
        "            text_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['text_hidden_neurons']\n",
        "        self.text_hidden_layers = nn.Sequential(*text_hidden_layers)\n",
        "        \n",
        "        self.text_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # Image model with MobileNet feature extractor\n",
        "        self.mobilenet_base = models.mobilenet_v2(pretrained=True)  # Use MobileNetV2\n",
        "        self.mobilenet_base.classifier = nn.Identity()  # Remove classifier for feature extraction\n",
        "        for param in self.mobilenet_base.parameters():\n",
        "            param.requires_grad = False  # Freeze MobileNet base layers\n",
        "        \n",
        "        # Additional hidden layers for image model (if specified)\n",
        "        vis_hidden_layers = []\n",
        "        input_size = 1280  # Flattened output from MobileNetV2\n",
        "        for _ in range(params['vis_no_hidden_layer']):\n",
        "            vis_hidden_layers.append(nn.Linear(input_size, params['vis_hidden_neurons']))\n",
        "            vis_hidden_layers.append(nn.ReLU())\n",
        "            vis_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            input_size = params['vis_hidden_neurons']\n",
        "        self.vis_hidden_layers = nn.Sequential(*vis_hidden_layers)\n",
        "\n",
        "        self.visual_repr = nn.Linear(input_size, params['repr_size'])\n",
        "        \n",
        "        # Final combined classifier\n",
        "        combined_size = 2 * params['repr_size']\n",
        "        final_hidden_layers = []\n",
        "        for _ in range(params['final_no_hidden_layer']):\n",
        "            final_hidden_layers.append(nn.Linear(combined_size, params['final_hidden_neurons']))\n",
        "            final_hidden_layers.append(nn.ReLU())\n",
        "            final_hidden_layers.append(nn.Dropout(params['dropout']))\n",
        "            combined_size = params['final_hidden_neurons']\n",
        "        self.final_hidden_layers = nn.Sequential(*final_hidden_layers)\n",
        "\n",
        "        self.classifier = nn.Linear(combined_size, 1)  # Binary classification\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, image):\n",
        "        # BERT forward pass\n",
        "        text_output = self.bert_layer(input_ids, attention_mask, token_type_ids)\n",
        "        text_output = self.text_hidden_layers(text_output)\n",
        "        text_repr = self.text_repr(text_output)\n",
        "        \n",
        "        # VGG-19 forward pass\n",
        "        image_features = self.mobilenet_base(image)\n",
        "        #image_flatten = image_features.view(image_features.size(0), -1)  # Flatten for fully connected layers\n",
        "        #image_output = self.vis_hidden_layers(image_flatten)\n",
        "        image_output = self.vis_hidden_layers(image_features )\n",
        "        visual_repr = self.visual_repr(image_output)\n",
        "        \n",
        "        # Concatenate text and visual representations\n",
        "        combined_repr = torch.cat((text_repr, visual_repr), dim=1)\n",
        "        combined_output = self.final_hidden_layers(combined_repr)\n",
        "        \n",
        "        # Final classifier layer\n",
        "        prediction = torch.sigmoid(self.classifier(combined_output))\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    def total_params(self):\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "# Example parameters (modify as needed)\n",
        "params = {\n",
        "    'bert_path':'bert-base-uncased',\n",
        "    'n_fine_tune_layers':1,\n",
        "    'bert_trainable' :False,\n",
        "    'text_no_hidden_layer':1,\n",
        "    'text_hidden_neurons':64,   #'text_hidden_neurons':768,\n",
        "    'dropout':0.4,\n",
        "    'repr_size':32,\n",
        "    'vis_no_hidden_layer':1,\n",
        "    'vis_hidden_neurons':64,   #'vis_hidden_neurons':2742,\n",
        "    'final_no_hidden_layer':1,\n",
        "    'final_hidden_neurons':35,\n",
        "    'optimizer':['adam'],\n",
        "    'batch_size':[1],\n",
        "    'epochs':[10]\n",
        "}\n",
        "model = NewsModel(params)\n",
        "print(f'Total number of parameters in the model: {model.total_params()}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
