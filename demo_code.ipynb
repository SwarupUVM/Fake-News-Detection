{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "CLIP model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swaru\\AppData\\Local\\Temp\\ipykernel_164216\\416672807.py:277: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model_final.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text features on CUDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting image features on CUDA...\n",
      "Test text features shape: torch.Size([1, 512])\n",
      "Test image features shape: torch.Size([1, 512])\n",
      "Test image features shape: (64, 64, 3)\n",
      "Test image tensor shape: torch.Size([1, 3, 64, 64])\n",
      "Extracting text features on CUDA...\n",
      "Extracting image features on CUDA...\n",
      "Test text features shape: torch.Size([1, 512])\n",
      "Test image features shape: torch.Size([1, 512])\n",
      "Test image features shape: (64, 64, 3)\n",
      "Test image tensor shape: torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import Text\n",
    "from PIL import Image, ImageTk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# MMoE model\n",
    "class MMoE(nn.Module):\n",
    "    def __init__(self, input_dim, expert_dim, num_experts, num_tasks, output_dim, num_fc_layers=2):\n",
    "        super(MMoE, self).__init__()\n",
    "        \n",
    "        self.num_tasks = num_tasks\n",
    "        self.num_experts = num_experts\n",
    "        \n",
    "        # Experts: A shared set of experts (fully connected layers)\n",
    "        self.experts = nn.ModuleList([self._build_fc_layers(input_dim, expert_dim, num_fc_layers) for _ in range(num_experts)])\n",
    "        \n",
    "        # Gates: One gating network per task\n",
    "        self.gates = nn.ModuleList([self._build_fc_layers(input_dim, num_experts, num_fc_layers) for _ in range(num_tasks)])\n",
    "        \n",
    "        # Output layers: One output layer per task\n",
    "        self.output_layers = nn.ModuleList([nn.Linear(expert_dim, output_dim) for _ in range(num_tasks)])\n",
    "\n",
    "    def _build_fc_layers(self, input_dim, output_dim, num_fc_layers):\n",
    "        layers = []\n",
    "        for i in range(num_fc_layers):\n",
    "            in_dim = input_dim if i == 0 else output_dim\n",
    "            layers.append(nn.Linear(in_dim, output_dim))\n",
    "            if i < num_fc_layers - 1:\n",
    "                layers.append(nn.ReLU())\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get expert outputs\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        # Stack expert outputs to shape (batch_size, num_experts, expert_dim)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
    "        \n",
    "        # Get the output for each task\n",
    "        outputs = []\n",
    "        for i in range(self.num_tasks):\n",
    "            gate_output = self.gates[i](x)  # (batch_size, num_experts)\n",
    "            gate_output = F.softmax(gate_output, dim=1)  # Normalize to get the weights\n",
    "            \n",
    "            # Weighted sum of expert outputs for the task\n",
    "            task_output = torch.sum(gate_output.unsqueeze(2) * expert_outputs, dim=1)  # (batch_size, expert_dim)\n",
    "            \n",
    "            # Apply task-specific output layer\n",
    "            task_output = self.output_layers[i](task_output)\n",
    "            outputs.append(task_output)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # (64, 32, 32)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Flatten(),  # (512 * 4 * 4 = 8192)\n",
    "            nn.Linear(512 * 4 * 4, 1),  # Fully connected layer to a single output\n",
    "            nn.Sigmoid()  # Output: probability for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Text and Image Similarity Model incorporating MMoE and Discriminator\n",
    "class TextImageSimilarityModel(nn.Module):\n",
    "    def __init__(self, text_image_input_dim, input_dim, embed_dim, num_fc_layers, expert_dim, num_experts, num_tasks, output_dim, num_fc_layers_mmoe):\n",
    "        super(TextImageSimilarityModel, self).__init__()\n",
    "        \n",
    "        # Text branch\n",
    "        self.text_layers = self._build_fc_layers(input_dim, embed_dim, num_fc_layers)\n",
    "        \n",
    "        # Image branch\n",
    "        self.image_layers = self._build_fc_layers(input_dim, embed_dim, num_fc_layers)\n",
    "        \n",
    "        self.mmoe = MMoE(text_image_input_dim, expert_dim, num_experts, num_tasks, output_dim, num_fc_layers_mmoe)\n",
    "        \n",
    "        self.discriminator = Discriminator()\n",
    "        \n",
    "        # Final fully connected layer for output\n",
    "        self.fc_layer = nn.Linear(1 + output_dim, 1)  # Adjust based on the concatenated dimensions\n",
    "        \n",
    "    def _build_fc_layers(self, input_dim, embed_dim, num_fc_layers):\n",
    "        layers = []\n",
    "        for i in range(num_fc_layers):\n",
    "            in_dim = input_dim if i == 0 else embed_dim\n",
    "            out_dim = embed_dim\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if i < num_fc_layers - 1:\n",
    "                layers.append(nn.ReLU())\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, text_features, image_features, image):\n",
    "        # Process text features\n",
    "        text_embedding = self.text_layers(text_features)\n",
    "        text_embedding = F.normalize(text_embedding, dim=-1)\n",
    "        \n",
    "        # Process image features\n",
    "        image_embedding = self.image_layers(image_features)\n",
    "        image_embedding = F.normalize(image_embedding, dim=-1)\n",
    "        \n",
    "        # Concatenate the text and image embeddings\n",
    "        combined_features = torch.cat([text_embedding, image_embedding], dim=-1)\n",
    "        \n",
    "        # Pass the combined features to MMoE\n",
    "        task_outputs = self.mmoe(combined_features)\n",
    "        \n",
    "        # For discriminator\n",
    "        out_dis = self.discriminator(image)\n",
    "        \n",
    "        # Concatenate task_outputs[0] and discriminator output\n",
    "        combined_out = torch.cat([task_outputs[0], out_dis], dim=-1)\n",
    "        \n",
    "        # Pass through the final FC layer\n",
    "        out = self.fc_layer(combined_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class FileLoaderApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text and Image Loader with CLIP\")\n",
    "\n",
    "        # Initialize CLIP model and preprocessing\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.clip_model, self.preprocess, self.device = self.create_clip_model_and_preprocessor()\n",
    "\n",
    "        # Frame for buttons\n",
    "        button_frame = tk.Frame(root)\n",
    "        button_frame.pack(pady=10)\n",
    "\n",
    "        # Buttons to load text and image\n",
    "        self.text_button = tk.Button(button_frame, text=\"Load Text File\", command=self.load_text)\n",
    "        self.text_button.grid(row=0, column=0, padx=10)\n",
    "\n",
    "        self.image_button = tk.Button(button_frame, text=\"Load Image File\", command=self.load_image)\n",
    "        self.image_button.grid(row=0, column=1, padx=10)\n",
    "\n",
    "        # Text area for displaying text content\n",
    "        self.text_area = Text(root, height=5, width=50)\n",
    "        self.text_area.pack(pady=10)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Buttons for feature extraction\n",
    "        # self.extract_text_button = tk.Button(root, text=\"Extract Text Features\", command=self.extract_text_features)\n",
    "        # self.extract_text_button.pack(pady=5)\n",
    "\n",
    "        # self.extract_image_button = tk.Button(root, text=\"Extract Image Features\", command=self.extract_image_features)\n",
    "        # self.extract_image_button.pack(pady=5)\n",
    "        \n",
    "        self.load_model_button = tk.Button(root, text=\"Check News\", command=self.load_text_image_similarity_model)\n",
    "        self.load_model_button.pack(pady=5)\n",
    "\n",
    "\n",
    "        # Labels for showing extracted features\n",
    "        self.text_features_label = tk.Label(root, text=\"Text Features: (Not Extracted)\", wraplength=400, justify=\"left\")\n",
    "        self.text_features_label.pack(pady=5)\n",
    "\n",
    "        self.image_features_label = tk.Label(root, text=\"Image Features: (Not Extracted)\", wraplength=400, justify=\"left\")\n",
    "        self.image_features_label.pack(pady=5)\n",
    "        \n",
    "        self.model_data_label = tk.Label(root, text=\"No model\", wraplength=400, justify=\"left\")\n",
    "        self.model_data_label.pack(pady=5)\n",
    "\n",
    "        # Canvas for displaying the image\n",
    "        self.image_canvas = tk.Canvas(root, width=400, height=300, bg=\"gray\")\n",
    "        self.image_canvas.pack(pady=10)\n",
    "\n",
    "        # Label for showing image path\n",
    "        self.image_label = tk.Label(root, text=\"No image loaded\", fg=\"blue\")\n",
    "        self.image_label.pack(pady=5)\n",
    "\n",
    "        # Variables to hold loaded text and image\n",
    "        self.loaded_text = None\n",
    "        self.loaded_image = None\n",
    "\n",
    "    def create_clip_model_and_preprocessor(self):\n",
    "        \"\"\"Initialize the CLIP model and preprocessing pipeline for images.\"\"\"\n",
    "        print(\"Loading CLIP model...\")\n",
    "        model, preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        print(\"CLIP model loaded.\")\n",
    "        return model, preprocess, self.device\n",
    "\n",
    "    def load_text(self):\n",
    "        \"\"\"Open a file dialog to select a text file and display its content.\"\"\"\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        if file_path:\n",
    "            with open(file_path, \"r\") as file:\n",
    "                self.loaded_text = file.read()\n",
    "            self.text_area.delete(\"1.0\", tk.END)  # Clear existing content\n",
    "            self.text_area.insert(tk.END, self.loaded_text)  # Insert file content\n",
    "\n",
    "    def load_image(self):\n",
    "        \"\"\"Open a file dialog to select an image file and display it.\"\"\"\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png;*.bmp\")])\n",
    "        if file_path:\n",
    "            self.image_label.config(text=file_path)\n",
    "\n",
    "            # Load and display the image\n",
    "            img = Image.open(file_path)\n",
    "            self.loaded_image = np.array(img)  # Save NumPy array for feature extraction\n",
    "            img.thumbnail((400, 400))  # Resize to fit canvas\n",
    "            img = ImageTk.PhotoImage(img)\n",
    "\n",
    "            self.image_canvas.delete(\"all\")  # Clear existing image\n",
    "            self.image_canvas.create_image(200, 200, anchor=tk.CENTER, image=img)\n",
    "            self.image_canvas.image = img  # Keep a reference to avoid garbage collection\n",
    "\n",
    "\n",
    "\n",
    "    def extract_text_features(self):\n",
    "        \"\"\"Extract features from the loaded text using CLIP on CUDA.\"\"\"\n",
    "        if not self.loaded_text:\n",
    "            self.text_features_label.config(text=\"No text loaded to extract features!\")\n",
    "            return\n",
    "\n",
    "        # Tokenize and encode the text using the CLIP model\n",
    "        print(\"Extracting text features on CUDA...\")\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize([self.loaded_text]).to(self.device)  # Move to CUDA\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "\n",
    "            # Normalize the features\n",
    "            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "                    # Display the text features\n",
    "            self.text_features_label.config(text=\"Text Features is extracted\")\n",
    "            return text_features\n",
    "        \n",
    "    def load_text_image_similarity_model(self,device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Load and initialize the TextImageSimilarityModel.\n",
    "\n",
    "        Args:\n",
    "            text_image_input_dim (int): Dimensionality of the input for the MMoE module.\n",
    "            input_dim (int): Input dimension for the feature extraction layers.\n",
    "            embed_dim (int): Dimensionality of the embeddings in feature extraction layers.\n",
    "            num_fc_layers (int): Number of fully connected layers in feature extraction.\n",
    "            expert_dim (int): Dimensionality of the expert layers in MMoE.\n",
    "            num_experts (int): Number of experts in MMoE.\n",
    "            num_tasks (int): Number of tasks for the MMoE model.\n",
    "            output_dim (int): Output dimension for each task.\n",
    "            num_fc_layers_mmoe (int): Number of fully connected layers in the MMoE model.\n",
    "            device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "\n",
    "        Returns:\n",
    "            TextImageSimilarityModel: The loaded and initialized model.\n",
    "        \"\"\"\n",
    "        # Instantiate the model\n",
    "        model = TextImageSimilarityModel(text_image_input_dim=512, input_dim=512, embed_dim=256, num_fc_layers=4, \n",
    "                                 expert_dim=64, num_experts=8, num_tasks=2, output_dim=1, num_fc_layers_mmoe=2).to(device)\n",
    "        model.load_state_dict(torch.load(\"model_final.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        if self.text_area.get(\"1.0\", \"end-1c\").strip():  # Check if the content is not empty\n",
    "            self.loaded_text = self.text_area.get(\"1.0\", \"end-1c\")\n",
    "\n",
    "        \n",
    "        \"\"\"Extract features from the loaded text using CLIP on CUDA.\"\"\"\n",
    "        if not self.loaded_text:\n",
    "            self.text_features_label.config(text=\"No text loaded to extract features!\")\n",
    "            return\n",
    "\n",
    "        # Tokenize and encode the text using the CLIP model\n",
    "        test_text_features = []\n",
    "        print(\"Extracting text features on CUDA...\")\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize([self.loaded_text]).to(self.device)  # Move to CUDA\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "\n",
    "            # Normalize the features\n",
    "            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "                    # Display the text features\n",
    "            test_text_features = text_features.to(device, dtype=torch.float32) \n",
    "            self.text_features_label.config(text=\"Text Features is extracted\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        test_image_features = []\n",
    "        \n",
    "        \"\"\"Extract features from the loaded image using CLIP on CUDA.\"\"\"\n",
    "        if self.loaded_image is None:\n",
    "            self.image_features_label.config(text=\"No image loaded to extract features!\")\n",
    "            return\n",
    "\n",
    "        # Process the image and extract features\n",
    "        print(\"Extracting image features on CUDA...\")\n",
    "        image_features = self.extract_image_features_from_np_single(\n",
    "            self.clip_model, self.loaded_image, self.preprocess, self.device\n",
    "        )\n",
    "        test_image_features=image_features.to(device, dtype=torch.float32) \n",
    "        \n",
    "\n",
    "        # Display the image features\n",
    "        self.image_features_label.config(text=\"Image Features is extracted\")\n",
    "        \n",
    "        \n",
    "        image = self.loaded_image\n",
    "        \n",
    "        #Resize images to 64x64 using OpenCV\n",
    "        test_imagesX = np.array(cv2.resize(image, (64, 64)))\n",
    "               \n",
    "        test_imagesX = test_imagesX.astype(np.float32) / 255.0  # Normalize images (scale pixel values to [0, 1])\n",
    "        # Convert train_imagesX to PyTorch tensor and move to device\n",
    "\n",
    "        # Check dimensions\n",
    "        print(f\"Test text features shape: {test_text_features.shape}\")\n",
    "        print(f\"Test image features shape: {test_image_features.shape}\")\n",
    "        print(f\"Test image features shape: {test_imagesX.shape}\")\n",
    "        \n",
    "       \n",
    "        test_imagesX_tensor = torch.from_numpy(test_imagesX)\n",
    "        test_imagesX_tensor = test_imagesX_tensor.unsqueeze(0).permute(0, 3, 1, 2).to(device) \n",
    "        print(f\"Test image tensor shape: {test_imagesX_tensor.shape}\")\n",
    "        \n",
    "         # Forward pass\n",
    "        with torch.no_grad():\n",
    "            output = model(test_text_features, test_image_features, test_imagesX_tensor)\n",
    "            #print(f\"Output: {output.item()}\")\n",
    "            probabilities = output.squeeze().cpu().numpy()  # Convert to numpy\n",
    "            predictions = (probabilities >= 0.5).astype(int)  # Binary classification\n",
    "            # Compare with label\n",
    "            #print(f\"Predicted label: {'Real' if predictions >=0.5 else 'Fake'}\")\n",
    "\n",
    "        # Compare with label\n",
    "        #print(f\"Actual label: {test_label[idx].item()}\")\n",
    "        \n",
    "        # idx = 1800  # Select the index of the data point you want to test\n",
    "        # # Load single data point\n",
    "        # test_text_features = torch.load('clip_test_text_features.pt').to(device, dtype=torch.float32)\n",
    "        # test_image_features = torch.load('clip_test_image_features.pt').to(device, dtype=torch.float32)\n",
    "        # test_label = torch.load('clip_test_label.pt').to(device, dtype=torch.float32)  # Test label\n",
    "\n",
    "        # test_imagesX = np.load('traslate_test_images_64.npy')\n",
    "        # test_imagesX = test_imagesX.astype(np.float32) / 255.0  # Normalize images (scale pixel values to [0, 1])\n",
    "        # # Convert train_imagesX to PyTorch tensor and move to device\n",
    "        # test_imagesX_tensor = torch.from_numpy(test_imagesX).permute(0, 3, 1, 2).to(device)  # (batch_size, C, H, W)\n",
    "\n",
    "        # idx = 1800  # Select the index of the data point you want to test\n",
    "        # # Check dimensions\n",
    "        # print(f\"Test text features shape: {test_text_features[idx].shape}\")\n",
    "        # print(f\"Test image features shape: {test_image_features[idx].shape}\")\n",
    "        # print(f\"Test image tensor shape: {test_imagesX_tensor[idx].shape}\")\n",
    "        \n",
    "\n",
    "        # # Forward pass\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(test_text_features[idx].unsqueeze(0), test_image_features[idx].unsqueeze(0), test_imagesX_tensor[idx].unsqueeze(0))\n",
    "        #     print(f\"Output: {output.item()}\")\n",
    "        #     probabilities = output.squeeze().cpu().numpy()  # Convert to numpy\n",
    "        #     predictions = (probabilities >= 0.5).astype(int)  # Binary classification\n",
    "        #     # Compare with label\n",
    "        #     print(f\"Predicted label: {'Real' if predictions >=0.5 else 'Fake'}\")\n",
    "\n",
    "        # # Compare with label\n",
    "        # print(f\"Actual label: {test_label[idx].item()}\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Display the image features\n",
    "        self.model_data_label.config(text=f\"The model is loaded. The model output: {output.item()}. \\nPredicted label: {'Real' if predictions >=0.5 else 'Fake'}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def extract_image_features(self):\n",
    "        \"\"\"Extract features from the loaded image using CLIP on CUDA.\"\"\"\n",
    "        if self.loaded_image is None:\n",
    "            self.image_features_label.config(text=\"No image loaded to extract features!\")\n",
    "            return\n",
    "\n",
    "        # Process the image and extract features\n",
    "        print(\"Extracting image features on CUDA...\")\n",
    "        image_features = self.extract_image_features_from_np_single(\n",
    "            self.clip_model, self.loaded_image, self.preprocess, self.device\n",
    "        )\n",
    "\n",
    "        # Display the image features\n",
    "        self.image_features_label.config(text=\"Image Features is extracted\")\n",
    "        return image_features\n",
    "\n",
    "\n",
    "\n",
    "    def extract_image_features_from_np_single(self, model, image, preprocess, device='cpu'):\n",
    "        \"\"\"\n",
    "        Extract image features using CLIP for a single NumPy array image.\n",
    "        Args:\n",
    "            model: CLIP model.\n",
    "            image: Single NumPy array (H, W, C).\n",
    "            preprocess: CLIP image preprocessing pipeline.\n",
    "            device: Device to run the extraction ('cpu' or 'cuda').\n",
    "        Returns:\n",
    "            Normalized image features as a PyTorch tensor.\n",
    "        \"\"\"\n",
    "        # Convert NumPy array to PIL image and preprocess\n",
    "        img_pil = Image.fromarray(np.uint8(image)).convert('RGB')  # Convert to PIL image\n",
    "        processed_image = preprocess(img_pil).unsqueeze(0).to(device)  # Preprocess and move to device\n",
    "\n",
    "        # Pass through the model to extract features\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(processed_image)\n",
    "            # Normalize the features\n",
    "            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "# Initialize the GUI application\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = FileLoaderApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
